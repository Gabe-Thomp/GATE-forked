{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fp = 'path/to/results.csv'\n",
        "df = pd.read_csv(fp)\n",
        "df.rename(columns={'Unnamed: 0': 'profile'}, inplace=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_df(df):\n",
        "    \"\"\"Rename the profile column and parse list columns.\"\"\"\n",
        "    df = df.copy()\n",
        "    if 'Unnamed: 0' in df.columns:\n",
        "        df = df.rename(columns={'Unnamed: 0': 'profile'})\n",
        "    cols = [\n",
        "        'interaction_time', 'interaction_num_turns', 'interaction_total_char_length',\n",
        "        'accuracy', 'AUCROC', 'correct_prob',\n",
        "        'accuracy_relative', 'AUCROC_relative', 'correct_prob_relative'\n",
        "    ]\n",
        "    for col in cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "    return df\n",
        "\n",
        "def plot_metric_over_interactions(df, metric, ax=None):\n",
        "    \"\"\"Plot a metric over interactions for each profile.\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "    for _, row in df.iterrows():\n",
        "        turns = row['interaction_num_turns']\n",
        "        values = row[metric]\n",
        "        ax.plot(turns, values, marker='o', label=f'Profile {row[\"profile\"]}')\n",
        "    ax.set_xlabel('interaction_num_terms')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.set_title(f'{metric} over interactions')\n",
        "    ax.legend()\n",
        "\n",
        "def compare_profiles(df, metric, turn=-1, ax=None):\n",
        "    \"\"\"Compare profiles for a given metric at a specific interaction step.\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "    values = []\n",
        "    labels = []\n",
        "    for _, row in df.iterrows():\n",
        "        data = row[metric]\n",
        "        val = data[turn] if isinstance(data, list) else data\n",
        "        labels.append(row['profile'])\n",
        "        values.append(val)\n",
        "    ax.bar(labels, values)\n",
        "    ax.set_xlabel('profile')\n",
        "    ax.set_ylabel(metric)\n",
        "    step = turn if turn >= 0 else 'final'\n",
        "    ax.set_title(f'{metric} at step {step}')\n",
        "    plt.xticks(rotation=45)\n",
        "    return pd.DataFrame({'profile': labels, metric: values})\n",
        "\n",
        "def plot_metric_average(df, metric, shade_std=True, ax=None):\n",
        "    \"\"\"Average the metric across profiles and plot it over interaction rounds.\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "    turns = np.array(df['interaction_num_turns'].tolist()[0])\n",
        "    values = np.vstack(df[metric].tolist())\n",
        "    mean_vals = values.mean(axis=0)\n",
        "    std_vals = values.std(axis=0)\n",
        "    ax.plot(turns, mean_vals, marker='o', label=f'Average {metric}')\n",
        "    if shade_std:\n",
        "        ax.fill_between(turns, mean_vals - std_vals, mean_vals + std_vals, alpha=0.3)\n",
        "    ax.set_xlabel('interaction_num_terms')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.set_title(f'Average {metric} over interactions')\n",
        "    ax.legend()\n",
        "\n",
        "def compare_dfs_average(dfs, metric, labels=None, shade_std=True, ax=None):\n",
        "    \"\"\"Compare multiple DataFrames on a metric averaged across profiles.\"\"\"\n",
        "    ax = ax or plt.gca()\n",
        "    if labels is None:\n",
        "        labels = [f'df{i+1}' for i in range(len(dfs))]\n",
        "    for df, label in zip(dfs, labels):\n",
        "        turns = np.array(df['interaction_num_turns'].tolist()[0])\n",
        "        values = np.vstack(df[metric].tolist())\n",
        "        mean_vals = values.mean(axis=0)\n",
        "        std_vals = values.std(axis=0)\n",
        "        ax.plot(turns, mean_vals, marker='o', label=label)\n",
        "        if shade_std:\n",
        "            ax.fill_between(turns, mean_vals - std_vals, mean_vals + std_vals, alpha=0.3)\n",
        "    ax.set_xlabel('interaction_num_terms')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.set_title(f'{metric} comparison')\n",
        "    ax.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "df = preprocess_df(df)\n",
        "plot_metric_over_interactions(df, 'accuracy')\n",
        "plot_metric_average(df, 'accuracy')\n",
        "plt.show()\n",
        "compare_profiles(df, 'accuracy')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "gate",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
